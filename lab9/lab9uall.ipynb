{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EDd-bpg0nJpj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import zipfile\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# see the lab documentation for details of the unet models\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPrq2jSnJpk"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C8elyoCunJpl"
   },
   "outputs": [],
   "source": [
    "# RCNN convnet1 \n",
    "class convnet1(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_channels=8):\n",
    "        super(convnet1, self).__init__()\n",
    "        self.activation = F.relu\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, n_channels, kernel_size= 1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(n_channels, 1, kernel_size= 1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "## Unet1\n",
    "class unet1(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_channels=8):\n",
    "        super(unet1, self).__init__()\n",
    "        self.activation = F.relu\n",
    "        \n",
    "        self.down1 = down_block(in_ch, n_channels)\n",
    "\n",
    "        self.bridge = down_block(in_ch=n_channels, out_ch=n_channels*2, max_pooling=False)\n",
    "\n",
    "        self.up1 = up_block(in_ch=n_channels*2, out_ch=n_channels)\n",
    "        self.final_conv = nn.Conv2d(n_channels, 1, kernel_size= 1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, skip1 = self.down1(x)\n",
    "        out, _ = self.bridge(out)\n",
    "        \n",
    "        out = self.up1(out, skip1)\n",
    "        out = self.final_conv(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "## Unet2\n",
    "class unet2(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_channels=8):\n",
    "        super(unet2, self).__init__()\n",
    "        \n",
    "        self.down1 = down_block(in_ch, n_channels)\n",
    "        self.down2 = down_block(in_ch=n_channels, out_ch=n_channels*2)\n",
    "\n",
    "        self.bridge= down_block(in_ch=n_channels*2, out_ch=n_channels*4, max_pooling=False)\n",
    "\n",
    "        self.up1 = up_block(in_ch=n_channels*4, out_ch=n_channels*2)\n",
    "        self.up2 = up_block(in_ch=n_channels*2, out_ch=n_channels)\n",
    "        self.final_conv = nn.Conv2d(n_channels, 1, kernel_size= 1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, skip1 = self.down1(x)\n",
    "        out, skip2 = self.down2(out)\n",
    "\n",
    "        out, _ = self.bridge(out)\n",
    "\n",
    "        out = self.up1(out, skip2)\n",
    "        out = self.up2(out, skip1)\n",
    "        out = self.final_conv(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class down_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dropout_prob=0, max_pooling=True):\n",
    "        super(down_block, self).__init__()\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.max_pooling = max_pooling\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.activation = F.relu\n",
    "        \n",
    "        # if dropout_prob > 0 add a dropout layer, with the variable dropout_prob as parameter\n",
    "        if dropout_prob > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n",
    "        if max_pooling:\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv = self.conv(inputs)\n",
    "        conv = self.bn(conv)\n",
    "        conv = self.activation(conv)\n",
    "        \n",
    "        if self.dropout_prob > 0:\n",
    "            conv = self.dropout(conv)\n",
    "            \n",
    "        next_layer = conv\n",
    "        skip_connection = conv\n",
    "        \n",
    "        if self.max_pooling:\n",
    "            next_layer = self.maxpool(conv)\n",
    "        \n",
    "        return next_layer, skip_connection\n",
    "   \n",
    "class up_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
    "        super(up_block, self).__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.activation = F.relu\n",
    "        \n",
    "    def forward(self, expansive_input, contractive_input=None):\n",
    "        up = self.up(expansive_input)\n",
    "        \n",
    "        merge = torch.cat([up, contractive_input], axis=1)\n",
    "        \n",
    "        out = self.conv(merge)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVoIH5RL64d4"
   },
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AD2Pk4ZMnJpn"
   },
   "outputs": [],
   "source": [
    "# utility function to display input image and its segmentation side by side\n",
    "def display_images(label, input_img, seg_img, pred_img=None):\n",
    "    # input_img = channels x width x height array\n",
    "    # seg_img = 1 x width x height array\n",
    "    # pred_img = 1 x width x height array (optional)\n",
    "    \n",
    "    input_img = np.moveaxis(input_img, 0, -1)\n",
    "    seg_img = np.squeeze(seg_img)\n",
    "    \n",
    "    if pred_img is None:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(6, 3))\n",
    "    axes[0].imshow(input_img)\n",
    "    axes[0].set_title('Input')\n",
    "    axes[1].imshow(seg_img, cmap='gray')\n",
    "    axes[1].set_title('Target')\n",
    "    \n",
    "    if pred_img is not None:\n",
    "        pred_img = np.squeeze(pred_img)\n",
    "        axes[2].imshow(pred_img, cmap='gray')\n",
    "        axes[2].set_title('Prediction')\n",
    "    fig.suptitle(label)\n",
    "\n",
    "    \n",
    "class SegmentationDataset():\n",
    "    \"\"\"\n",
    "    img_dir = directory containing images\n",
    "    labels = list of image labels\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, labels):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels  = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        img_id = label.split(\".\")[0][2:]\n",
    "        input_img_label = \"im%s.png\" % img_id\n",
    "        seg_img_label = \"imseg%s.png\" % img_id\n",
    "        input_img_path = os.path.join(self.img_dir, input_img_label)\n",
    "        seg_img_path = os.path.join(self.img_dir, seg_img_label)\n",
    "        \n",
    "        input_img = np.asarray(imageio.imread(input_img_path))\n",
    "        seg_img = np.asarray(imageio.imread(seg_img_path))\n",
    "\n",
    "        input_img = input_img / 255.0\n",
    "        input_img = np.moveaxis(input_img, -1, 0)\n",
    "        seg_img = seg_img / 255\n",
    "        seg_img = np.expand_dims(seg_img, axis=0)\n",
    "\n",
    "        return label, input_img, seg_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvjchQlBnJpo"
   },
   "source": [
    "## Download and Initialize the custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBAvDo3ynJpp",
    "outputId": "95a27603-466a-49ed-d727-1d5d9c3bfe66"
   },
   "outputs": [],
   "source": [
    "# If on JHUB use local dataset else download\n",
    "JHUB = 0\n",
    "\n",
    "#select the dataset\n",
    "#\n",
    "# 16 x 16 color digits\n",
    "#dsetn = \"lab9data1\"\n",
    "# 16 x 16 red digits\n",
    "#dsetn = \"lab9data1r\"\n",
    "# 16 x 16 red digits\n",
    "#dsetn = \"lab9data2\"\n",
    "# 16 x 16 red digits\n",
    "#dsetn = \"lab9data2r\"\n",
    "# 64 x 64 images with color digits\n",
    "dsetn = \"lab9data3\"\n",
    "# 64 x 64 images with red digits\n",
    "#dsetn = \"lab9data3r\"\n",
    "\n",
    "## On JHUB use downloaded dataet\n",
    "if  JHUB:\n",
    "   dset = \"/classes/ece5470/lab/lab9/\" + dsetn\n",
    "else:\n",
    "    dset = dsetn\n",
    "    url = 'https://www.via.cornell.edu/ece5470/' + dsetn + '.zip'\n",
    "    r = requests.get(url).content\n",
    "    z = zipfile.ZipFile(io.BytesIO(r))\n",
    "    z.extractall('./' + dset)\n",
    "\n",
    "labels_df = pd.read_csv(os.path.join(dset, \"labels.csv\"))\n",
    "labels = labels_df.iloc[:, 0].values\n",
    "\n",
    "print(\"Total number of images:\", len(labels))\n",
    "\n",
    "test_labels = labels[:100]\n",
    "val_labels = labels[100:200]\n",
    "#train_labels = labels[200:]\n",
    "train_labels = labels[200:700]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuzMxKs6nJpp"
   },
   "outputs": [],
   "source": [
    "# Load Images\n",
    "train_dataset = SegmentationDataset(dset, train_labels)\n",
    "val_dataset   = SegmentationDataset(dset, val_labels)\n",
    "test_dataset  = SegmentationDataset(dset, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "DpXsUuIsnJpp",
    "outputId": "e29b94e0-e032-4dbf-c017-95a83595398d"
   },
   "outputs": [],
   "source": [
    "# show example input image/segmentation pairs\n",
    "elist = iter(train_dataset)\n",
    "for idx in range(1):\n",
    "    label, input_img, target = next(elist)   \n",
    "    display_images(label, input_img, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fpAOZm1nJpq"
   },
   "source": [
    "# Training and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CiC8YEJnJpq"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch_idx, (label, data, target) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        target = target.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += (loss.item() * data.size(0) / len(train_loader.sampler))\n",
    "\n",
    "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
    "            print('  [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def eval(model, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            target = target.float()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            avg_loss += (loss.item() * data.size(0) / len(val_loader.sampler))\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Wu3T9bTnJpq"
   },
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMZVvwt9nJpq",
    "outputId": "c9bd16f2-f071-4dca-a546-4389b1722fd6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\"\"\" initialization \"\"\"\n",
    "model = convnet1()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\"\"\" cooking \"\"\"\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "start_time = datetime.now()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"------- Epoch\", epoch+1, \"-------\")\n",
    "    print(\"  Training\")\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "    print(\"     Loss =\", train_loss)\n",
    "    print(\"  Validation\")\n",
    "    val_loss   = eval(model, val_loader, loss_fn)\n",
    "    print(\"     Loss =\", val_loss)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\") # remember to change the filename if you don't want to overwrite the saved checkpoint\n",
    "        best_val_loss = val_loss\n",
    "print(\"Time Elapsed: {} seconds\".format((datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtUbSsU1nJpr"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7sVImUCnJpr"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "zRgeOPhinJpr",
    "outputId": "1f78b67c-7a92-460c-cc37-0b280b34d97b"
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYeaO_w3nJps"
   },
   "outputs": [],
   "source": [
    "def compute_dice_score(im1, im2):\n",
    "    # compute Dice score between 2 binary images im1 and im2\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape\")\n",
    "\n",
    "    denom = im1.sum() + im2.sum()\n",
    "    if denom == 0:\n",
    "        return None\n",
    "\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DjnHc-SnJps"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, data, target) in enumerate(test_loader):\n",
    "            data = data.float()\n",
    "            target = target.float()\n",
    "\n",
    "            output = torch.sigmoid(model(data))\n",
    "            output[output >= threshold] = 1 # binarize the prediction at threshold\n",
    "            output[output < threshold] = 0\n",
    "            \n",
    "            for i in range(target.shape[0]):\n",
    "                sample_label = label[i]\n",
    "                sample_data = data[i, :, :, :]\n",
    "                sample_target = target[i, :, :, :]\n",
    "                sample_pred = output[i, :, : , :]\n",
    "                \n",
    "                dice = compute_dice_score(sample_target, sample_pred)\n",
    "            \n",
    "                pred.append({\n",
    "                    \"label\": sample_label,\n",
    "                    \"input\": sample_data.numpy(),\n",
    "                    \"target\": sample_target.numpy(),\n",
    "                    \"pred\": sample_pred.numpy(),\n",
    "                    \"dice\": dice\n",
    "                })\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iO3NN2SCnJps",
    "outputId": "723e5f26-1b0b-41a6-b846-27eb9345bc54"
   },
   "outputs": [],
   "source": [
    "test_loader   = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "start_time = datetime.now()\n",
    "pred = predict(model, test_loader, threshold=0.05)\n",
    "print(\"Average Dice score =\", np.mean([sample[\"dice\"] for sample in pred]))\n",
    "print(\"Time Elapsed: {} seconds\".format((datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XxZkZWwLnJps",
    "outputId": "92af0881-0a69-4883-cb5e-fbd094187e86"
   },
   "outputs": [],
   "source": [
    "# show example results\n",
    "\n",
    "for idx in range(5):\n",
    "    random_sample = np.random.choice(pred)\n",
    "    display_images(\"%s - Dice = %.2f\" % (random_sample[\"label\"], random_sample[\"dice\"]), random_sample[\"input\"], random_sample[\"target\"], random_sample[\"pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader   = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "x=[0.05, 0.025, 0.05, 0.1, 0.12, 0.3, 0.5]     #x is the set of threshold values. You can modify it.\n",
    "y=[]\n",
    "start_time = datetime.now()\n",
    "for i in x:\n",
    "    pred = predict(model, test_loader, threshold=i)\n",
    "    mp=np.mean([sample[\"dice\"] for sample in pred])\n",
    "    print(\"Average Dice score for threshold\",i, \"=\",mp)\n",
    "    print(\"Time Elapsed: {} seconds\".format((datetime.now() - start_time).total_seconds()))\n",
    "    y.append(mp)\n",
    "\n",
    "#plotting Average Dice score based on given thrshold values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y,'-o')\n",
    "ax.set_ylabel('Average Dice score')\n",
    "ax.set_xlabel('Threshold')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
